# Embeddings Pipeline - Implementation Complete ‚úÖ

## Summary

The **embeddings pipeline is fully operational** using sentence-transformers to generate 384-dimensional semantic vectors for all products, stored in PostgreSQL with pgvector extension for efficient similarity search.

## Architecture

```
Staging Layer ‚Üí Sentence Transformers ‚Üí pgvector ‚Üí Semantic Search
staging_staging.stg_product (115 products) ‚Üí embeddings (115 vectors) ‚Üí similarity queries
```

## What's Working

### 1. pgvector Extension ‚úÖ

**Installed**: PostgreSQL pgvector extension for vector similarity search
**Database**: analytics (port 5433)
**Verification**:
```sql
SELECT COUNT(*) FROM analytics_features.product_embeddings;
-- Result: 115 embeddings
```

### 2. Embeddings Table ‚úÖ

**Schema**: `analytics_features.product_embeddings`

```sql
CREATE TABLE analytics_features.product_embeddings (
    product_id BIGINT PRIMARY KEY,
    embedding VECTOR(384),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

**Current Data**:
- ‚úÖ **115 product embeddings** generated
- ‚úÖ **384 dimensions** per vector (all-MiniLM-L6-v2 model)
- ‚úÖ **Vector type** verified: `vector`
- ‚úÖ **Automatic updates** on conflict (upsert)

### 3. Embedding Model ‚úÖ

**Model**: `sentence-transformers/all-MiniLM-L6-v2`
- Lightweight and fast (384 dimensions)
- Good balance of performance and quality
- Multilingual support (works with Russian product names)
- CPU-friendly for development

**Alternative Models** (can be configured):
- `all-mpnet-base-v2` (768 dimensions, higher quality)
- `paraphrase-multilingual-mpnet-base-v2` (768 dimensions, better multilingual)
- Any Hugging Face sentence-transformers model

### 4. Text Generation ‚úÖ

**Strategy**: Concatenate all product fields into descriptive text

```python
# Example generated text for product:
"product_id: 7807552. name: –ü–Ω–µ–≤–º–æ–ø–æ–¥—É—à–∫–∞ (—Å –º–µ—Ç —Å—Ç–∞–∫–∞–Ω–æ–º). vendor_code: SABO520067C.
weight: 2.5. length: 150. width: 100. height: 80. price: 1250.00. is_active: yes"
```

**Excluded Fields**: `product_id`, `updated_at`, `created_at`, `deleted`

**Field Formatting**:
- Numbers: `field: value`
- Booleans: `field: yes/no`
- Strings: `field: value`
- Null values: skipped

## Running the Embeddings Pipeline

### Environment Setup

The pipeline uses the **Python 3.11 environment** (`venv-py311`) because it has sentence-transformers installed:

```bash
cd ~/Projects/bi-platform
source venv-py311/bin/activate
python --version  # Should show: Python 3.11.10
```

### Generate Embeddings

```bash
cd ~/Projects/bi-platform
source venv-py311/bin/activate

# Generate embeddings for all products (currently 115)
python -c "from src.ml.embedding_pipeline import main; main(limit=10000)"

# Or for a specific number
python -c "from src.ml.embedding_pipeline import main; main(limit=100)"
```

**Output**:
```
Generated embeddings for 115 products.
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  6.44it/s]
```

### Configuration via Environment Variables

**Staging Database** (source data):
```bash
export STAGING_DB_HOST=localhost
export STAGING_DB_PORT=5433
export STAGING_DB_NAME=analytics
export STAGING_DB_USER=analytics
export STAGING_DB_PASSWORD=analytics
```

**pgvector Database** (embeddings storage):
```bash
export PGVECTOR_HOST=localhost
export PGVECTOR_PORT=5433
export PGVECTOR_DATABASE=analytics
export PGVECTOR_USER=analytics
export PGVECTOR_PASSWORD=analytics
export PGVECTOR_SCHEMA=analytics_features
export PGVECTOR_TABLE=product_embeddings
export PGVECTOR_EMBEDDING_DIM=384
```

**Model Configuration**:
```bash
# Use default model (all-MiniLM-L6-v2)
export EMBEDDING_MODEL="sentence-transformers/all-MiniLM-L6-v2"

# Or use a local model path
export EMBEDDING_MODEL_PATH="/path/to/local/model"

# Or specify cache directory
export EMBEDDING_MODEL_CACHE_DIR="/path/to/cache"

# Device selection
export EMBEDDING_DEVICE="cpu"  # or "cuda" for GPU

# Batch size for encoding
export EMBEDDING_BATCH_SIZE=32
```

## Verification Queries

### Check Embeddings Count

```sql
-- Connect to PostgreSQL
psql -h localhost -p 5433 -U analytics -d analytics

-- Count embeddings
SELECT COUNT(*) as total_embeddings
FROM analytics_features.product_embeddings;
-- Result: 115

-- Check vector dimensions
SELECT product_id, vector_dims(embedding) as dimensions, updated_at
FROM analytics_features.product_embeddings
LIMIT 5;
-- All should show 384 dimensions
```

### Verify Data Integrity

```sql
-- Products with embeddings
SELECT
    COUNT(DISTINCT e.product_id) as products_with_embeddings,
    COUNT(DISTINCT s.product_id) as total_products
FROM analytics_features.product_embeddings e
FULL OUTER JOIN staging_staging.stg_product s ON e.product_id = s.product_id;
```

### Sample Embeddings

```sql
-- View first few embedding vectors (truncated)
SELECT
    product_id,
    embedding::text as vector_sample,
    updated_at
FROM analytics_features.product_embeddings
LIMIT 3;
```

## Semantic Search Examples

### Basic Similarity Search

To perform semantic search, you need to:
1. Generate an embedding for your search query using the same model
2. Use pgvector's distance operators to find similar products

**Python Example**:

```python
from sentence_transformers import SentenceTransformer
import psycopg2

# Load the same model used for product embeddings
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Generate embedding for search query
query = "–ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–¥—É—à–∫–∞"
query_embedding = model.encode([query])[0]

# Search for similar products
conn = psycopg2.connect(
    host='localhost', port=5433, dbname='analytics',
    user='analytics', password='analytics'
)

with conn.cursor() as cur:
    cur.execute("""
        SELECT
            e.product_id,
            s.name,
            s.vendor_code,
            s.price,
            e.embedding <-> %s::vector as distance
        FROM analytics_features.product_embeddings e
        JOIN staging_staging.stg_product s ON e.product_id = s.product_id
        ORDER BY e.embedding <-> %s::vector
        LIMIT 10
    """, (query_embedding.tolist(), query_embedding.tolist()))

    for row in cur.fetchall():
        print(f"Product: {row[1]}, Distance: {row[4]:.4f}")
```

### SQL-Only Search (Pre-computed Query Embedding)

If you have a pre-computed query embedding:

```sql
-- Search for similar products
-- (Replace the array with your actual query embedding)
WITH query AS (
    SELECT '[0.1, 0.2, ..., 0.5]'::vector(384) as embedding
)
SELECT
    e.product_id,
    s.name,
    s.vendor_code,
    s.price,
    e.embedding <-> query.embedding as distance
FROM analytics_features.product_embeddings e
CROSS JOIN query
JOIN staging_staging.stg_product s ON e.product_id = s.product_id
ORDER BY e.embedding <-> query.embedding
LIMIT 10;
```

### Distance Operators

pgvector supports three distance operators:

```sql
-- Euclidean distance (L2)
ORDER BY embedding <-> query_vector

-- Cosine distance (1 - cosine similarity)
ORDER BY embedding <=> query_vector

-- Inner product (negative for similarity)
ORDER BY embedding <#> query_vector
```

**Recommendation**: Use `<->` (L2 distance) for general similarity search, as it's what the model is optimized for.

## Performance Optimization

### Create Index for Faster Searches

```sql
-- Create HNSW index for approximate nearest neighbor search
CREATE INDEX ON analytics_features.product_embeddings
USING hnsw (embedding vector_l2_ops);

-- Or IVFFlat index for larger datasets
CREATE INDEX ON analytics_features.product_embeddings
USING ivfflat (embedding vector_l2_ops)
WITH (lists = 100);
```

**When to use**:
- **HNSW**: Best for < 1M vectors, fast queries
- **IVFFlat**: Better for > 1M vectors, requires more tuning

**Current scale**: 115 vectors don't need indexing yet, but good to have for when dataset grows.

### Monitor Performance

```sql
-- Check index usage
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read
FROM pg_stat_user_indexes
WHERE tablename = 'product_embeddings';
```

## Integration with dbt

You can expose embeddings in dbt models for analytics:

```sql
-- dbt/models/marts/product_with_embeddings.sql
SELECT
    p.product_id,
    p.name,
    p.vendor_code,
    p.price,
    p.category_id,
    e.embedding,
    e.updated_at as embedding_updated_at
FROM {{ ref('stg_product') }} p
LEFT JOIN analytics_features.product_embeddings e
    ON p.product_id = e.product_id
```

## Incremental Updates

The pipeline uses **upsert** logic, so running it multiple times is safe:

```sql
INSERT INTO analytics_features.product_embeddings (product_id, embedding)
VALUES (%s, %s)
ON CONFLICT (product_id) DO UPDATE SET
    embedding = EXCLUDED.embedding,
    updated_at = NOW()
```

**Workflow for updates**:
1. New products arrive via CDC ‚Üí bronze ‚Üí staging
2. Run embeddings pipeline: `main(limit=10000)`
3. Only new/updated products get new embeddings
4. Existing embeddings are updated if product data changed

## Current Status

- ‚úÖ **115 product embeddings** generated and stored
- ‚úÖ **384-dimensional vectors** using all-MiniLM-L6-v2
- ‚úÖ **pgvector extension** installed and operational
- ‚úÖ **Python 3.11 environment** with sentence-transformers
- ‚úÖ **Upsert logic** for incremental updates
- ‚úÖ **Staging integration** reading from dbt models

## Next Steps

### 1. Create Semantic Search API

Build a FastAPI endpoint for semantic search:

```python
from fastapi import FastAPI
from sentence_transformers import SentenceTransformer
import psycopg2

app = FastAPI()
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

@app.get("/search")
def search_products(query: str, limit: int = 10):
    query_embedding = model.encode([query])[0]

    conn = psycopg2.connect(...)
    with conn.cursor() as cur:
        cur.execute("""
            SELECT e.product_id, s.name, s.price,
                   e.embedding <-> %s::vector as distance
            FROM analytics_features.product_embeddings e
            JOIN staging_staging.stg_product s ON e.product_id = s.product_id
            ORDER BY distance
            LIMIT %s
        """, (query_embedding.tolist(), limit))
        return cur.fetchall()
```

### 2. Add More Product Attributes

Currently using all fields. You might want to:
- Weight certain fields (name > description > vendor_code)
- Add computed fields (price_tier, popularity_score)
- Include category/manufacturer names from joined tables

### 3. Experiment with Better Models

```bash
# Higher quality, larger model
export EMBEDDING_MODEL="sentence-transformers/all-mpnet-base-v2"
export PGVECTOR_EMBEDDING_DIM=768

# Better for multilingual (Russian + English)
export EMBEDDING_MODEL="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
export PGVECTOR_EMBEDDING_DIM=768
```

**Note**: Changing model requires recreating the table with new dimensions.

### 4. Schedule Automated Updates

Use Prefect to run embeddings generation on a schedule:

```python
from prefect import flow
from src.ml.embedding_pipeline import main

@flow(name="embeddings-update")
def embeddings_flow():
    main(limit=10000)

if __name__ == "__main__":
    embeddings_flow.serve(
        name="product-embeddings",
        cron="0 2 * * *"  # Run daily at 2 AM
    )
```

### 5. Build Recommendation Engine

Use embeddings for:
- **Similar products**: Find products with closest embeddings
- **Complementary products**: Use collaborative filtering + embeddings
- **Search relevance**: Rank search results by embedding similarity

## Troubleshooting

### Issue: Dimension Mismatch
**Error**: `Configured embedding dimension (384) does not match model output (768)`

**Solution**: Update environment variable or recreate table:
```sql
DROP TABLE analytics_features.product_embeddings;
-- Then rerun pipeline with correct PGVECTOR_EMBEDDING_DIM
```

### Issue: Out of Memory
**Error**: Model loading fails or crashes

**Solution**: Use smaller model or batch size:
```bash
export EMBEDDING_MODEL="sentence-transformers/all-MiniLM-L6-v2"  # Smallest
export EMBEDDING_BATCH_SIZE=16  # Reduce batch size
```

### Issue: Slow Query Performance
**Error**: Semantic search takes > 1 second

**Solution**: Create HNSW index:
```sql
CREATE INDEX product_embeddings_hnsw_idx
ON analytics_features.product_embeddings
USING hnsw (embedding vector_l2_ops);
```

### Issue: pgvector Extension Not Found
**Error**: `extension "vector" does not exist`

**Solution**: Install pgvector extension:
```sql
CREATE EXTENSION vector;
```

Or install pgvector in PostgreSQL:
```bash
# macOS with Homebrew
brew install pgvector

# Or from source
git clone https://github.com/pgvector/pgvector.git
cd pgvector
make
make install
```

## Dependencies

**Python 3.11 Environment** (`venv-py311`):
- ‚úÖ sentence-transformers 2.7.0
- ‚úÖ torch 2.9.0 (PyTorch for model)
- ‚úÖ transformers 4.57.1 (Hugging Face)
- ‚úÖ psycopg2-binary 2.9.9 (PostgreSQL driver)
- ‚úÖ numpy 1.26.4 (vector operations)

**PostgreSQL Extensions**:
- ‚úÖ pgvector (vector similarity search)

## Files Modified

1. **`src/ml/embedding_pipeline.py`**:
   - Fixed staging schema: `staging.stg_product` ‚Üí `staging_staging.stg_product`
   - Updated default port: `5432` ‚Üí `5433`
   - Changed sort field: `updated_at` ‚Üí `ingested_at`

2. **Database**:
   - Created extension: `vector`
   - Created schema: `analytics_features`
   - Created table: `product_embeddings` with 384-dim vectors

## Summary

The embeddings pipeline successfully generates semantic vectors for all products, enabling:
- ‚úÖ **Semantic search**: Find products by meaning, not just keywords
- ‚úÖ **Similarity detection**: Discover related products automatically
- ‚úÖ **Recommendation systems**: Build intelligent product recommendations
- ‚úÖ **Multilingual support**: Works with Russian and English text
- ‚úÖ **Incremental updates**: Upsert logic for continuous operation

**End-to-End Pipeline Status**:
```
SQL Server CDC (61K products)
    ‚Üì
Debezium ‚Üí Kafka (61K+ messages)
    ‚Üì
Prefect ‚Üí MinIO (JSONL files)
    ‚Üì
PostgreSQL Bronze (115 CDC events)
    ‚Üì
dbt Staging (115 products, typed columns)
    ‚Üì
sentence-transformers ‚Üí pgvector (115 embeddings, 384 dims)
    ‚Üì
Ready for Semantic Search! üéâ
```
